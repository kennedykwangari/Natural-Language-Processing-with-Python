{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Natural Language Processing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quora Insincere Questions Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quora is a platform that empowers people to learn from each other. \n",
    "\n",
    "On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
    "\n",
    "We are tasked to develop NLP models that identify and flag insincere questions.\n",
    "\n",
    "To date, Quora has employed both machine learning and manual review to address this problem.\n",
    "\n",
    "#### Build scalable methods to derect toxic and misleading content to improve online conversations and combat online trolls at scale\n",
    "\n",
    "Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Required Libraries and Getting our Systems Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #  import TF-idf vectorizer\n",
    "from sklearn.utils import shuffle # to shuffle the data \n",
    "import random # import random\n",
    "import sklearn # import sklearn\n",
    "import nltk # import nltk\n",
    "from nltk.corpus import stopwords #import stop words\n",
    "import re # import regular expression\n",
    "from nltk.tokenize import word_tokenize # import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1306122 entries, 0 to 1306121\n",
      "Data columns (total 3 columns):\n",
      "qid              1306122 non-null object\n",
      "question_text    1306122 non-null object\n",
      "target           1306122 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 29.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Loading the Train and Test Sets\n",
    "\n",
    "train = pd.read_csv('train.csv.zip')\n",
    "test  = pd.read_csv('test.csv.zip')\n",
    "full_data = [train, test]\n",
    "print (train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding / Pre-Processing and  Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Text Pre-processing of text data\n",
    "        Lower casing\n",
    "        Punctuation removal\n",
    "        Stopwords removal\n",
    "        Frequent words removal\n",
    "        Rare words removal\n",
    "        Spelling correction\n",
    "    Tokenization\n",
    "        Stemming\n",
    "        Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (1306122, 4)\n",
      "Columns are: Index(['qid', 'question_text', 'target', 'Contains_capitalized'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',train.shape)\n",
    "print('Columns are:',train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1306122 entries, 0 to 1306121\n",
      "Data columns (total 4 columns):\n",
      "qid                     1306122 non-null object\n",
      "question_text           1306122 non-null object\n",
      "target                  1306122 non-null int64\n",
      "Contains_capitalized    1306122 non-null bool\n",
      "dtypes: bool(1), int64(1), object(2)\n",
      "memory usage: 31.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = train['target'].values\n",
    "\n",
    "np.unique(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.306122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.187018e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.409197e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "count  1.306122e+06\n",
       "mean   6.187018e-02\n",
       "std    2.409197e-01\n",
       "min    0.000000e+00\n",
       "25%    0.000000e+00\n",
       "50%    0.000000e+00\n",
       "75%    0.000000e+00\n",
       "max    1.000000e+00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Description of the Non-Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1306122</td>\n",
       "      <td>1306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1306122</td>\n",
       "      <td>1306122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>bcb70a87949a0df29882</td>\n",
       "      <td>What's the best way to invest your money and g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "count                1306122   \n",
       "unique               1306122   \n",
       "top     bcb70a87949a0df29882   \n",
       "freq                       1   \n",
       "\n",
       "                                            question_text  \n",
       "count                                             1306122  \n",
       "unique                                            1306122  \n",
       "top     What's the best way to invest your money and g...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include = ['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the Tweeps Tweet Contains Capitalized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def contains_capitalized_word(s):\n",
    "    for w in s.split():\n",
    "        if w.isupper():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "train['Contains_capitalized'] = train[\"question_text\"].apply(contains_capitalized_word)\n",
    "\n",
    "value_counts = train[\"Contains_capitalized\"].value_counts().to_dict()\n",
    "fig, ax = plt.subplots()\n",
    "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
    "           colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'}, startangle=45)\n",
    "_ = ax.axis('equal')\n",
    "_ = ax.set_title('Does the Question Text Contain Capitalized Word?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.696326\n",
       "True     0.303674\n",
       "Name: Contains_capitalized, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Contains_capitalized\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the Target Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us look at the distribution of the target variable to understand more about the imbalance and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225312, 80810)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sincere_q = (train['target'] == 0).sum()\n",
    "insincere_q = (train['target'] == 1).sum()\n",
    "\n",
    "sincere_q, insincere_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(train['target'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Quora Insincere Questions Distribution Histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.81298224821265% of questions are sincere and 6.187017751787352% are insincere\n"
     ]
    }
   ],
   "source": [
    "rate_sincere_q = (sincere_q/len(train['target']))*100\n",
    "rate_insincere_q = (insincere_q/len(train['target']))*100\n",
    "rate_sincere_q, rate_insincere_q\n",
    "print( '{}% of questions are sincere and {}% are insincere'.format(rate_sincere_q, rate_insincere_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 6% of the training data are insincere questions (target=1) and rest of them are sincere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-Processing Text Data in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the text data are cleaned by following below steps.\n",
    "\n",
    "Remove punctuations\n",
    "\n",
    "Tokenization - Converting a sentence into list of words\n",
    "\n",
    "Removing stopwords\n",
    "\n",
    "Lammetization/stemming - Tranforming any form of a word to its root word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Tweet_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you have an adopted dog how would you encou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does velocity affect time Does velocity af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target                                        Tweet_punct  \n",
       "0       0  How did Quebec nationalists see their province...  \n",
       "1       0  Do you have an adopted dog how would you encou...  \n",
       "2       0  Why does velocity affect time Does velocity af...  \n",
       "3       0  How did Otto von Guericke used the Magdeburg h...  \n",
       "4       0  Can I convert montra helicon D to a mountain b...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "train['Tweet_punct'] = train['question_text'].apply(lambda x: remove_punct(x))\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of segmenting running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation.\n",
    "\n",
    "'''Tokenization refers to dividing the text into a sequence of words or sentences.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>[how, did, quebec, nationalists, see, their, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you have an adopted dog how would you encou...</td>\n",
       "      <td>[do, you, have, an, adopted, dog, how, would, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "\n",
       "   target                                        Tweet_punct  \\\n",
       "0       0  How did Quebec nationalists see their province...   \n",
       "1       0  Do you have an adopted dog how would you encou...   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [how, did, quebec, nationalists, see, their, p...  \n",
       "1  [do, you, have, an, adopted, dog, how, would, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "train['Tweet_tokenized'] = train['Tweet_punct'].apply(lambda x: tokenization(x.lower()))\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Many sentences and paragraphs include words that have very little meaning or value. These words include “a,” “and,” “an,” and “the.” Stop word removal is a process of removing these words from a sentence or stream of words.\n",
    "\n",
    "Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>[how, did, quebec, nationalists, see, their, p...</td>\n",
       "      <td>[quebec, nationalists, see, province, nation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you have an adopted dog how would you encou...</td>\n",
       "      <td>[do, you, have, an, adopted, dog, how, would, ...</td>\n",
       "      <td>[adopted, dog, would, encourage, people, adopt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does velocity affect time Does velocity af...</td>\n",
       "      <td>[why, does, velocity, affect, time, does, velo...</td>\n",
       "      <td>[velocity, affect, time, velocity, affect, spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "\n",
       "   target                                        Tweet_punct  \\\n",
       "0       0  How did Quebec nationalists see their province...   \n",
       "1       0  Do you have an adopted dog how would you encou...   \n",
       "2       0  Why does velocity affect time Does velocity af...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [how, did, quebec, nationalists, see, their, p...   \n",
       "1  [do, you, have, an, adopted, dog, how, would, ...   \n",
       "2  [why, does, velocity, affect, time, does, velo...   \n",
       "\n",
       "                                       Tweet_nonstop  \n",
       "0      [quebec, nationalists, see, province, nation]  \n",
       "1  [adopted, dog, would, encourage, people, adopt...  \n",
       "2  [velocity, affect, time, velocity, affect, spa...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "train['Tweet_nonstop'] = train['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming / lexicon normalization is the process of reducing noise in a word. It reduces inflection. For example, the word “fishing” has a stem word “fish.” Stemming is used to simplify a word down to its base meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. \n",
    "\n",
    "For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>[how, did, quebec, nationalists, see, their, p...</td>\n",
       "      <td>[quebec, nationalists, see, province, nation]</td>\n",
       "      <td>[quebec, nationalist, see, provinc, nation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you have an adopted dog how would you encou...</td>\n",
       "      <td>[do, you, have, an, adopted, dog, how, would, ...</td>\n",
       "      <td>[adopted, dog, would, encourage, people, adopt...</td>\n",
       "      <td>[adopt, dog, would, encourag, peopl, adopt, shop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does velocity affect time Does velocity af...</td>\n",
       "      <td>[why, does, velocity, affect, time, does, velo...</td>\n",
       "      <td>[velocity, affect, time, velocity, affect, spa...</td>\n",
       "      <td>[veloc, affect, time, veloc, affect, space, ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "\n",
       "   target                                        Tweet_punct  \\\n",
       "0       0  How did Quebec nationalists see their province...   \n",
       "1       0  Do you have an adopted dog how would you encou...   \n",
       "2       0  Why does velocity affect time Does velocity af...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [how, did, quebec, nationalists, see, their, p...   \n",
       "1  [do, you, have, an, adopted, dog, how, would, ...   \n",
       "2  [why, does, velocity, affect, time, does, velo...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0      [quebec, nationalists, see, province, nation]   \n",
       "1  [adopted, dog, would, encourage, people, adopt...   \n",
       "2  [velocity, affect, time, velocity, affect, spa...   \n",
       "\n",
       "                                       Tweet_stemmed  \n",
       "0        [quebec, nationalist, see, provinc, nation]  \n",
       "1  [adopt, dog, would, encourag, peopl, adopt, shop]  \n",
       "2  [veloc, affect, time, veloc, affect, space, ge...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "train['Tweet_stemmed'] = train['Tweet_nonstop'].apply(lambda x: stemming(x))\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>Tweet_punct</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_nonstop</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>[how, did, quebec, nationalists, see, their, p...</td>\n",
       "      <td>[quebec, nationalists, see, province, nation]</td>\n",
       "      <td>[quebec, nationalist, see, provinc, nation]</td>\n",
       "      <td>[quebec, nationalist, see, province, nation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>Do you have an adopted dog how would you encou...</td>\n",
       "      <td>[do, you, have, an, adopted, dog, how, would, ...</td>\n",
       "      <td>[adopted, dog, would, encourage, people, adopt...</td>\n",
       "      <td>[adopt, dog, would, encourag, peopl, adopt, shop]</td>\n",
       "      <td>[adopted, dog, would, encourage, people, adopt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why does velocity affect time Does velocity af...</td>\n",
       "      <td>[why, does, velocity, affect, time, does, velo...</td>\n",
       "      <td>[velocity, affect, time, velocity, affect, spa...</td>\n",
       "      <td>[veloc, affect, time, veloc, affect, space, ge...</td>\n",
       "      <td>[velocity, affect, time, velocity, affect, spa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "\n",
       "   target                                        Tweet_punct  \\\n",
       "0       0  How did Quebec nationalists see their province...   \n",
       "1       0  Do you have an adopted dog how would you encou...   \n",
       "2       0  Why does velocity affect time Does velocity af...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [how, did, quebec, nationalists, see, their, p...   \n",
       "1  [do, you, have, an, adopted, dog, how, would, ...   \n",
       "2  [why, does, velocity, affect, time, does, velo...   \n",
       "\n",
       "                                       Tweet_nonstop  \\\n",
       "0      [quebec, nationalists, see, province, nation]   \n",
       "1  [adopted, dog, would, encourage, people, adopt...   \n",
       "2  [velocity, affect, time, velocity, affect, spa...   \n",
       "\n",
       "                                       Tweet_stemmed  \\\n",
       "0        [quebec, nationalist, see, provinc, nation]   \n",
       "1  [adopt, dog, would, encourag, peopl, adopt, shop]   \n",
       "2  [veloc, affect, time, veloc, affect, space, ge...   \n",
       "\n",
       "                                    Tweet_lemmatized  \n",
       "0       [quebec, nationalist, see, province, nation]  \n",
       "1  [adopted, dog, would, encourage, people, adopt...  \n",
       "2  [velocity, affect, time, velocity, affect, spa...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "train['Tweet_lemmatized'] = train['Tweet_nonstop'].apply(lambda x: lemmatizer(x))\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Cleaned Text Data to CSV\n",
    "\n",
    "train.to_csv(\"CleanedSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "Cleaning data in single line through passing clean_text in the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CleanedSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1225312\n",
       "1      80810\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306122 Number of question_text has 179523 words\n"
     ]
    }
   ],
   "source": [
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(df['question_text'])\n",
    "print('{} Number of question_text has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(countVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    " \n",
    "#Importing Required Packages\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced  Text Processing in  Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advance Text Processing Techniques \n",
    "        N-grams\n",
    "        Term Frequency\n",
    "        Inverse Document Frequency\n",
    "        Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "        Bag of Words\n",
    "        Hashing with HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Bag of Words to Text Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Bag of Words Vectorization to the Text Data\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words= 'english')\n",
    "bow = bow_vectorizer.fit_transform(df['Tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sklearn has a separate function to directly obtain TF and IDF \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(df['Tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(df['Tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the TF-IDF Vectorization to the  Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying TF-IDF Vectorization to the Text Data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['Tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing with HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large.\n",
    "\n",
    "This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n",
    "\n",
    "A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. \n",
    "A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=100)\n",
    "vector = vectorizer.fit_transform(df['Tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams \n",
    "\n",
    "'''N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. \n",
    "\n",
    "Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "TextBlob(df['question_text'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "'''Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encourage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>an</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adopted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shop?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adopt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Do</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf\n",
       "0         you   2\n",
       "1         not   1\n",
       "2        have   1\n",
       "3         and   1\n",
       "4   encourage   1\n",
       "5         how   1\n",
       "6          an   1\n",
       "7     adopted   1\n",
       "8       shop?   1\n",
       "9       adopt   1\n",
       "10     people   1\n",
       "11      would   1\n",
       "12       dog,   1\n",
       "13         Do   1\n",
       "14         to   1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf1 = (df['question_text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse Document Frequency\n",
    "'''\n",
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "The IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>2</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>3.044522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "      <td>2.708050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encourage</td>\n",
       "      <td>1</td>\n",
       "      <td>7.786967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "      <td>3.583519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>an</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adopted</td>\n",
       "      <td>1</td>\n",
       "      <td>8.207402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shop?</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adopt</td>\n",
       "      <td>1</td>\n",
       "      <td>7.021976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>people</td>\n",
       "      <td>1</td>\n",
       "      <td>3.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog,</td>\n",
       "      <td>1</td>\n",
       "      <td>9.688064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Do</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf\n",
       "0         you   2  1.791759\n",
       "1         not   1  3.044522\n",
       "2        have   1  2.708050\n",
       "3         and   1  1.386294\n",
       "4   encourage   1  7.786967\n",
       "5         how   1  3.583519\n",
       "6          an   1  0.000000\n",
       "7     adopted   1  8.207402\n",
       "8       shop?   1  2.890372\n",
       "9       adopt   1  7.021976\n",
       "10     people   1  3.218876\n",
       "11      would   1  3.258097\n",
       "12       dog,   1  9.688064\n",
       "13         Do   1  2.890372\n",
       "14         to   1  0.693147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(df[df['question_text'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you</td>\n",
       "      <td>2</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>3.583519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.044522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>2.708050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encourage</td>\n",
       "      <td>1</td>\n",
       "      <td>7.786967</td>\n",
       "      <td>7.786967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>how</td>\n",
       "      <td>1</td>\n",
       "      <td>3.583519</td>\n",
       "      <td>3.583519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>an</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adopted</td>\n",
       "      <td>1</td>\n",
       "      <td>8.207402</td>\n",
       "      <td>8.207402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shop?</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.890372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adopt</td>\n",
       "      <td>1</td>\n",
       "      <td>7.021976</td>\n",
       "      <td>7.021976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>people</td>\n",
       "      <td>1</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>would</td>\n",
       "      <td>1</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>3.258097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog,</td>\n",
       "      <td>1</td>\n",
       "      <td>9.688064</td>\n",
       "      <td>9.688064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Do</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.890372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  tf       idf     tfidf\n",
       "0         you   2  1.791759  3.583519\n",
       "1         not   1  3.044522  3.044522\n",
       "2        have   1  2.708050  2.708050\n",
       "3         and   1  1.386294  1.386294\n",
       "4   encourage   1  7.786967  7.786967\n",
       "5         how   1  3.583519  3.583519\n",
       "6          an   1  0.000000  0.000000\n",
       "7     adopted   1  8.207402  8.207402\n",
       "8       shop?   1  2.890372  2.890372\n",
       "9       adopt   1  7.021976  7.021976\n",
       "10     people   1  3.218876  3.218876\n",
       "11      would   1  3.258097  3.258097\n",
       "12       dog,   1  9.688064  9.688064\n",
       "13         Do   1  2.890372  2.890372\n",
       "14         to   1  0.693147  0.693147"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn has a separate function to directly obtain TF and IDF \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EDA and later Modelling Purposes, it might be a good idea to create some metafeatures. The metafeatures that we'll create are:\n",
    "\n",
    "Number of words in the text\n",
    "\n",
    "Number of unique words in the text\n",
    "\n",
    "Number of characters in the text\n",
    "\n",
    "Number of stopwords\n",
    "\n",
    "Number of punctuations\n",
    "\n",
    "Number of upper case words\n",
    "\n",
    "Number of title case words\n",
    "\n",
    "Average length of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Special Characters\n",
    "\n",
    "Number of Numerics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of questions in train is 13.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of questions in train is {0:.0f}.'.format(np.mean(df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of questions in train is 71.\n"
     ]
    }
   ],
   "source": [
    "print('Average character length of questions in train is {0:.0f}.'.format(np.mean(df['question_text'].apply(lambda x: len(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word length of questions in train is 134.\n"
     ]
    }
   ],
   "source": [
    "print('Max word length of questions in train is {0:.0f}.'.format(np.max(df['question_text'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic  Feature Extraction with Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the text ##\n",
    "df[\"num_words\"] = df[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "df[\"num_unique_words\"] = df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "df[\"num_chars\"] = df[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "df[\"num_stopwords\"] = df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "df[\"num_punctuations\"] =df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "df[\"num_words_upper\"] = df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "df[\"num_words_title\"] = df[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "df[\"mean_word_len\"] = df[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Count Based And Demographical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generating Count Based And Demographical Features\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "print(\">> Generating Count Based And Demographical Features\")\n",
    "for df in ([df]):\n",
    "    df['length'] = df['question_text'].apply(lambda x : len(x))\n",
    "    df['capitals'] = df['question_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['length']),axis=1)\n",
    "    df['num_exclamation_marks'] = df['question_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['question_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['question_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['question_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df['num_sad'] = df['question_text'].apply(lambda comment: sum(comment.count(w) for w in (':-<', ':()', ';-()', ';(')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's have a glance at new features ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>num_smilies</th>\n",
       "      <th>num_sad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.615385</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>67</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.444444</td>\n",
       "      <td>57</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>113</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>69</td>\n",
       "      <td>3</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         13                13         72              7                 1   \n",
       "1         16                15         81              9                 2   \n",
       "2         10                 8         67              3                 2   \n",
       "3          9                 9         57              3                 1   \n",
       "4         15                15         77              8                 1   \n",
       "5         10                10         72              3                 2   \n",
       "6         18                17        113             10                 2   \n",
       "7         14                14         69              7                 2   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  length  capitals  \\\n",
       "0                0                2       4.615385      72         2   \n",
       "1                0                1       4.125000      81         1   \n",
       "2                0                2       5.800000      67         2   \n",
       "3                0                4       5.444444      57         4   \n",
       "4                2                3       4.200000      77         3   \n",
       "5                0                6       6.300000      72         6   \n",
       "6                0                2       5.333333     113         2   \n",
       "7                1                3       4.000000      69         3   \n",
       "\n",
       "   caps_vs_length  num_exclamation_marks  num_question_marks  num_punctuation  \\\n",
       "0        0.027778                      0                   1                0   \n",
       "1        0.012346                      0                   1                1   \n",
       "2        0.029851                      0                   2                0   \n",
       "3        0.070175                      0                   1                0   \n",
       "4        0.038961                      0                   1                0   \n",
       "5        0.083333                      0                   1                1   \n",
       "6        0.017699                      0                   1                1   \n",
       "7        0.043478                      0                   1                1   \n",
       "\n",
       "   num_symbols  words_vs_unique  num_smilies  num_sad  \n",
       "0            0         1.000000            0        0  \n",
       "1            0         0.937500            0        0  \n",
       "2            0         0.800000            0        0  \n",
       "3            0         1.000000            0        0  \n",
       "4            0         1.000000            0        0  \n",
       "5            0         1.000000            0        0  \n",
       "6            0         0.944444            0        0  \n",
       "7            0         1.000000            0        0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[8:]].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a Clean and Feature Engineered dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Cleaned Text Data to CSV\n",
    "\n",
    "df.to_csv(\"FeatureENg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeature = pd.read_csv('FeatureENg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1306122 entries, 0 to 1306121\n",
      "Data columns (total 26 columns):\n",
      "qid                      1306122 non-null object\n",
      "question_text            1306122 non-null object\n",
      "target                   1306122 non-null int64\n",
      "Tweet_punct              1306121 non-null object\n",
      "Tweet_tokenized          1306122 non-null object\n",
      "Tweet_nonstop            1306122 non-null object\n",
      "Tweet_stemmed            1306122 non-null object\n",
      "Tweet_lemmatized         1306122 non-null object\n",
      "num_words                1306122 non-null int64\n",
      "num_unique_words         1306122 non-null int64\n",
      "num_chars                1306122 non-null int64\n",
      "num_stopwords            1306122 non-null int64\n",
      "num_punctuations         1306122 non-null int64\n",
      "num_words_upper          1306122 non-null int64\n",
      "num_words_title          1306122 non-null int64\n",
      "mean_word_len            1306122 non-null float64\n",
      "length                   1306122 non-null int64\n",
      "capitals                 1306122 non-null int64\n",
      "caps_vs_length           1306122 non-null float64\n",
      "num_exclamation_marks    1306122 non-null int64\n",
      "num_question_marks       1306122 non-null int64\n",
      "num_punctuation          1306122 non-null int64\n",
      "num_symbols              1306122 non-null int64\n",
      "words_vs_unique          1306122 non-null float64\n",
      "num_smilies              1306122 non-null int64\n",
      "num_sad                  1306122 non-null int64\n",
      "dtypes: float64(3), int64(16), object(7)\n",
      "memory usage: 259.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dfeature.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
